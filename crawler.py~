import urllib2
from BeautifulSoup import *
from urlparse import urljoin

ignorewordset=set(['the','of','a','and','in','is','it'])

def crawl(self,pages,depth=2):

  for i in range(depth):
    
	  new pages=set()
	  for page in pages:
		  try:
			  c=urllib2.urlopen(page)
		  except:
			  print "Could not open %s" % page
			  continue
		  soup=BeautifulSoup(c.read())
		  self.addtoindex(page,soup)


		  links=soup('a')

		  for link in links:
			  if ('href' in dict(link.attrs)):
				  url = urljoin(page,link['href'])
				  if url.find("'") != -1: continue
				  url=url.split('#')[0]
				  if url[0:4]=='http' and not self.isindexed(url):
					  newpages.add(url)
				  linkText=self.gettextonly(link)
				  self.addlinkref(page,url,linkText)
			 self.dbcommit()
			 pages=newpages
